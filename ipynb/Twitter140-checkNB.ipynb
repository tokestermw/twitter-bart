{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Twitter Sentiment Analysis of #BARTStrike using nltk and scikit-learn"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Author: Motoki Wu ([@plusepsilon](https://twitter.com/plusepsilon))\n",
      "\n",
      "I streamed tweets from the [second BART strike](http://www.mapreport.com/na/west/ba/news/cities/bart_strike.html) to the end of that strike. Since [Twitter API](https://dev.twitter.com/docs/streaming-apis) has rate limits, I collected 180 tweets every five minutes, totaling to about 22000 tweets. Sentiment labeling was done by [Sentiment140](http://www.sentiment140.com). For this notebook, I am just interested in which words were influential in determining sentiment.\n",
      "\n",
      "Since I can't post the data anywhere, this notebook assumes I've collected the tweets and converted them into readable JSON format."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import display"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# install libraries\n",
      "import json\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
      "from nltk import sent_tokenize, word_tokenize, FreqDist, WordNetLemmatizer\n",
      "from nltk.corpus import stopwords"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# source data in\n",
      "with open(\"../data/polarity.json\", 'r') as f:\n",
      "    out = json.load(f)\n",
      "\n",
      "## filter out data with polarity neutral = 2\n",
      "d_ = [i for i in out['data'] if i['polarity'] != 2]\n",
      "\n",
      "del(out)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d_[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "{u'created_at': u'2013-10-18 01:31:38',\n",
        " u'id': 391013669116456962,\n",
        " u'meta': {u'language': u'en'},\n",
        " u'polarity': 0,\n",
        " u'text': u\"I'm disappointed Bart unions. You couldn't come up with a realistic counter offer? What a waste of 6 months. Commute = disaster #BARTstrike\"}"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# custom tokenizer\n",
      "## featurize for each tweet\n",
      "def featurize(tweet):\n",
      "    # tokenize into words\n",
      "    tokens = [word for sent in sent_tokenize(tweet) for word in word_tokenize(sent)]\n",
      "\n",
      "    # remove stopwords\n",
      "    stop = stopwords.words('english')\n",
      "    tokens = [token for token in tokens if token not in stop]\n",
      "\n",
      "    # remove words less than three letters\n",
      "    tokens = [word for word in tokens if len(word) >= 3]\n",
      "\n",
      "    # lower capitalization\n",
      "    tokens = [word.lower() for word in tokens]\n",
      "\n",
      "    # lemmatize\n",
      "    lmtzr = WordNetLemmatizer()\n",
      "    tokens = [lmtzr.lemmatize(word) for word in tokens]\n",
      "\n",
      "    return tokens\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# wrapper to run the classifier\n",
      "def run_classifier(train, test, vctrzr, clsfr):\n",
      "    # vectorize with above featurize() function and also vectorize test set with training fit\n",
      "    X_train = vctrzr.fit_transform([i[0] for i in train])\n",
      "    X_test = vctrzr.transform([i[0] for i in test])\n",
      "\n",
      "    # fit the classifier with training data\n",
      "    clsfr.fit(X_train, [i[1] for i in train])\n",
      "    \n",
      "    # grab accuracy\n",
      "    scr = clsfr.score(X_test, [i[1] for i in test])\n",
      "\n",
      "    # grab important features\n",
      "    imp_features = sorted(zip(clsfr.coef_[0], vctrzr.get_feature_names()))\n",
      "    \n",
      "    return scr, imp_features\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "posneg = {0: 'neg', 4: 'pos'}\n",
      "train, test = train_test_split([(i['text'], posneg[i['polarity']]) for i in d_], \n",
      "                               test_size = .2, \n",
      "                               random_state = 10)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = CountVectorizer(tokenizer = featurize, binary = True)\n",
      "classifier = MultinomialNB()\n",
      "\n",
      "score, imp_features = run_classifier(train, test, vectorizer, classifier)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = TfidfVectorizer(tokenizer = featurize)\n",
      "classifier = LinearSVC()\n",
      "\n",
      "score, imp_features = run_classifier(train, test, vectorizer, classifier)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "display(score)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "0.89518413597733715"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Best features that predicted negative sentiment."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "display(imp_features[0:20])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "[(-2.8357465163165063, u\"n't\"),\n",
        " (-2.4201593740395526, u'work'),\n",
        " (-2.2099005503662612, u'sad'),\n",
        " (-2.134341230708428, u'traffic'),\n",
        " (-1.9487734183304863, u'bad'),\n",
        " (-1.9410436588253621, u'suck'),\n",
        " (-1.7625886379782472, u'hate'),\n",
        " (-1.5710481225736643, u'still'),\n",
        " (-1.5688234447934066, u'leaving'),\n",
        " (-1.5295688143615913, u'stuck'),\n",
        " (-1.5139701088397268, u'bus'),\n",
        " (-1.5013235053943947, u'damn'),\n",
        " (-1.4788029325321641, u'horrible'),\n",
        " (-1.3002409121097998, u'missing'),\n",
        " (-1.2643054498836144, u'stupid'),\n",
        " (-1.2439906270740047, u'lost'),\n",
        " (-1.2330823454521014, u'ugh'),\n",
        " (-1.2300572586379617, u'missed'),\n",
        " (-1.207876020758087, u'left'),\n",
        " (-1.1925205756393427, u'sorry')]"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Best features that predicted positive sentiment."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "display(imp_features[-21:-1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "[(1.2572713812534602, u'abc7newsbayarea'),\n",
        " (1.2848970173168219, u'nicely'),\n",
        " (1.3114599170734345, u'yay'),\n",
        " (1.3141989313725035, u'sfbayferry'),\n",
        " (1.3331475843002596, u'ready'),\n",
        " (1.3626098784763807, u'safe'),\n",
        " (1.4427299291680631, u'awesome'),\n",
        " (1.533406184356106, u'hey'),\n",
        " (1.5904100695696874, u'thank'),\n",
        " (1.6002919329257885, u'great'),\n",
        " (1.6171697845738462, u'finally'),\n",
        " (1.6259904844181883, u'sfbart'),\n",
        " (1.6489921434356374, u'love'),\n",
        " (1.6657530957108404, u'thx'),\n",
        " (1.6724072807099553, u'best'),\n",
        " (1.6875918587710637, u'excited'),\n",
        " (1.728437377530184, u'good'),\n",
        " (1.7847619469736122, u'nice'),\n",
        " (2.2958159850061266, u'happy'),\n",
        " (2.9483929041967141, u'http')]"
       ]
      }
     ],
     "prompt_number": 34
    }
   ],
   "metadata": {}
  }
 ]
}